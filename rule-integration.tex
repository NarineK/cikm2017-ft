% @Author: Xiaocheng Tang
% @Date:   2017-05-11 22:01:21
% @Last Modified by:   Xiaocheng Tang
% @Last Modified time: 2017-05-11 22:02:26
% !TEX root = ./abstract.tex

In this work we utilize generative models to create new training labels and de-noise the data (\cite{genmodels}).
The model applies generative processes to labeling functions and learns their structural correlations and conflicts.
We incorporate numerous labeling functions as a supply of domain knowledge for weak supervision. Table (\ref{tab:numrules})
lists some of the categories and the number of rules associated with them. Labeling functions assign 1 if the product belongs
to the category, -1 if it does not and 0 if it is uncertain\cite{genmodels}.

The generative model optimizes noise aware loss function and estimates probabilities $p(Label, \lambda)$,
where $Label$ is the category we want to predict, $\lambda = f(x)$ is a list of values assigned by labeling
function $f$ to product $x$.  Since current implementation of generative model is binary we apply it for each
category separately and combine resulted labeled data in a final output. Picture (\ref{fig:genmodel_pred_probs}) shows
the distribution of marginal probabilities for several classes. We assume that the product belongs to certain category if
prediction probability is greater or equal to 0.5.

We use unlabeled product items to train generative model and apply learned model to manually labeled test data and measure
prediction accuracy (\ref{tab:evaluation}).
The train and test sets contains respectively approx. 100k and 20k product items.

@TODO create rule overlap chart

Ultimately, we combine generated and manually labeled training sets to train a discriminative model to predict accurate product categories.

\begin{table*}
  \caption{Examples of the category labels and the total numbers of rules in each category}
  \label{tab:numrules}
  \begin{tabular}{ll}
    \toprule
    Category & \#Rules\\
    \midrule
    Boots \textgreater Women &	88	\\
    Boots \textgreater Men	& 36 \\
 	Travel \textgreater Hotel & 30 \\
 	Home \textgreater LampShades & 19 \\
 	Home \textgreater Other & 11 \\
 	Grocery \textgreater Nutrition Drinks & 11 \\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}
  \caption{Evaluation results for several categories on sub-sampled data}
  \label{tab:evaluation}
  \begin{tabular}{lllllll}
    \toprule
    Category & Presicion & Recall & F1 & Accuraccy & Neg Class Accuracy & Pos Class Accuracy\\
    \midrule 
    Boots \textgreater Women	& 98\% & 100\% & 99\% & 98\% & 100\% & 98\% \\
    Boots \textgreater Men &	95\%	& 100\% & 97\% & 98\% & 100\% & 95\% \\
    \bottomrule
  \end{tabular}
\end{table*}
\begin{figure}[th]
   \includegraphics[width=0.5\textwidth]{resources/genmodel_pred_probs}
   \caption{The distribution of marginal probabilities}
   \label{fig:genmodel_pred_probs}
   \centering
\end{figure}


/*
weak supervision, in which training labels are noisy and may be from
multiple, potentially overlapping sources
To address this, we model the labeling functions as a generative
process, which lets us automatically de-noise the resulting training set by learning the accuracies of
the labeling functions along with their correlation structure. In turn, we use this model of the training
set to optimize a stochastic version of the loss function of the discriminative model that we desire to train. 
*/