\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
% \usepackage[pdf]{pstricks}
% \usepackage{graphicx}
% \usepackage{epstopdf}
\usepackage[update,prepend]{epstopdf}
\usepackage[algo2e,vlined,ruled,linesnumbered]{algorithm2e}
\usepackage{subcaption}

\input{math_symbol_define}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
  Paso, Texas USA}
\acmYear{1997}
\copyrightyear{2016}

\acmPrice{15.00}


\begin{document}
\title{SIG Proceedings Paper in LaTeX Format}
\titlenote{Produces the permission block, and
  copyright information}
\subtitle{Extended Abstract}
\subtitlenote{The full version of the author's guide is available as
  \texttt{acmart.pdf} document}


% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{B. Trovato et al.}


\begin{abstract}
Purchase item categorization is both a critical step in mining user online shopping behavior and a key enabler of 
many downstream analytical tasks such as user personalization and trend prediction.
Training a categorization system, however, often involves large amount of parameters due to the large-scale nature of both the feature (ngram) and the output space (in-house taxonomy). 
Moreover, using supervised learning alone sometimes is not enough to achieve a satisfactory accuracy due to the high cost of obtaining labeled data and the noisy labeling process, e.g., crowd sourcing.   

We therefore propose an end-to-end categorization system that requires only weak supervision by incorporating domain knowledge and business rules through generative modeling. We show that, by explicitly modeling the way domain knowledge generate labels, we are able to make use of the large amount of unlabeled data during training in a more cost-efficient way. 
During training, the system obtains latent representation of both the item and the category in a low-dimensional dense vector form. We show that this not only helps reduce the trainable parameters from O(|feature| x |output|) to O(|feature| + |output|) but also leads to high accuracy by capturing the syntactic and semantic feature / category relationships. We then show how to train the system efficiently by exploiting sparsity and multi-threading with momentum.
Experimentally we apply the system to machine-generated receipts from millions of user accounts with hundreds of categories. The results show state-of-the-art categorization performance.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}


% \keywords{ACM proceedings, \LaTeX, text tagging}


\maketitle

\section{Introduction}
\input{intro}

\section{End to End Learning for Item Categorization}
\input{end-to-end}

\section{Rule Integration Through Generative Modeling}
\input{rule-integration}

\section{Data Pipeline}
\input{data-pipe}

\section{Results}
\input{results}

\section{Conclusions}
\input{conclusions}


\bibliographystyle{ACM-Reference-Format}
\bibliography{sigproc}

\end{document}
